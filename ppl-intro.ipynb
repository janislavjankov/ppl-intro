{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to PPL\n",
    "\n",
    "To start: the \"hello world\" of statistics - coin flips with biased coin (where do you find one of these?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate biased coin flips and compute the likelihood. The coin flips are draws from Bernoulli distribution parameterized with h, the likelihood of the independent trials is:\n",
    "$$\n",
    "p(X\\mid h) = successes^h failures^{1 - h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coin_flips(n_samples: int, p_heads: float):\n",
    "    return np.cast[np.int32](np.random.uniform(size=n_samples) < p_heads)\n",
    "\n",
    "def likelihood(samples, p_heads):\n",
    "    n_heads = np.sum(samples == 1)\n",
    "    return p_heads ** n_heads * (1 - p_heads) ** (len(samples) - n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_p_heads = 0.5\n",
    "samples = coin_flips(n_samples=100, p_heads=true_p_heads)\n",
    "print(\"samples:\\n{}\".format(samples))\n",
    "print(\"heads: {}/{}, ratio: {}\".format(np.sum(samples), len(samples), np.sum(samples) / len(samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "posterior:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(H \\mid X) &= \\frac{p(X \\mid H) p(H)}{p(X)}\\\\\n",
    "&= \\frac{p(X \\mid H) p(H)}{\\sum {p(X \\mid H) p(H)}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_h = np.linspace(0.0, 1.0, num=100)\n",
    "q_h = likelihood(samples=samples, p_heads=p_h)\n",
    "q_h = q_h / np.sum(q_h)\n",
    "plt.plot(p_h, q_h)\n",
    "plt.vlines(x=true_p_heads, ymin=0.0, ymax=np.max(q_h), linestyle=\"dotted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce some more complex priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(alpha, beta):\n",
    "    return np.random.beta(a=alpha, b=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = [prior(alpha=20, beta=20) for _ in range(1000)]\n",
    "plt.hist(priors, bins=20)\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter tensorflow probability (which includes Edward2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import edward2 as ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coin flips with Beta prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(alpha, beta):\n",
    "    return ed.Beta(concentration0=alpha, concentration1=beta, name=\"prob\")\n",
    "\n",
    "def model(alpha, beta, size):\n",
    "    p = prior(alpha=alpha, beta=beta)\n",
    "    return ed.Bernoulli(probs=p * tf.ones(shape=size), name=\"outcomes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = tf.placeholder(tf.float32)\n",
    "pdf_prior = prior(alpha=20.0, beta=20.0).distribution.prob(value=x_)\n",
    "n_samples_ = tf.placeholder(tf.int32)\n",
    "samples = model(alpha=20.0, beta=20.0, size=n_samples_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1.0, 100)\n",
    "with tf.Session() as sess:\n",
    "    samples_value, prior_pdf_value = sess.run([samples, pdf_prior], {x_: x, n_samples_: 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, prior_pdf_value);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform inference - find the posterior distribution of our hidden variable, which we called \"prob\".\n",
    "\n",
    "First we will compute the log joint distribution of the model, which is needed by most of the inference algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outcomes = 20\n",
    "outcomes_ = tf.placeholder(tf.float32, shape=[n_outcomes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_joint = ed.make_log_joint_fn(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line created a function that can compute the log joint distribution given values of all the model variables. As we are seeking to infer the distribution of \"prob\" we will explicitly condition on the observations by plugging in \"outcomes_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_log_prob_fn(logit_prob):\n",
    "    return log_joint(20.0, 20.0, outcomes_.shape[0], prob=tf.nn.sigmoid(logit_prob), outcomes=outcomes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcomes = np.cast[np.int32](np.random.uniform(size=n_outcomes) < .1)\n",
    "outcomes = np.zeros(n_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some sampling ...\n",
    "\n",
    "Using black box algorithm: Metropolis-Hastings, which performs random walk and accepts the state based on the ratio of the current and next state probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_kernel = tfp.mcmc.MetropolisHastings(\n",
    "    inner_kernel=tfp.mcmc.UncalibratedRandomWalk(target_log_prob_fn=target_log_prob_fn),\n",
    ")\n",
    "states, kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=1000,\n",
    "    current_state=[.0],\n",
    "    kernel=mh_kernel,\n",
    "    num_burnin_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    s_, kr_ = sess.run([states, kernel_results], {\n",
    "        outcomes_: outcomes\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(sigmoid(s_[0]), bins=20)\n",
    "plt.xlim([0, 1.0])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Variational Inference? With the current state of Edward2 / TFP you need to manually define the ELBO loss and optimize it. The elegance of Edward just doesn't come through, hence I'll completely avoid it.\n",
    "\n",
    "https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/edward2/Upgrading_From_Edward_To_Edward2.md#probabilistic-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian linear regression\n",
    "\n",
    "Let's step it up a notch with bayesian linear regression:\n",
    "\n",
    "$$\n",
    "y = a x + b + \\epsilon \\\\\n",
    "\\epsilon \\sim N(0, \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(low=-10.0, high=10.0, size=20)\n",
    "true_a = 0.5\n",
    "true_b = 7\n",
    "true_sigma = 1.0\n",
    "\n",
    "y = np.random.normal(loc=true_a * x + true_b, scale=true_sigma)\n",
    "plt.scatter(x, y, marker=\".\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model with explicit priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(data):\n",
    "    a = ed.Normal(loc=0.0, scale=1.0, name=\"a\")\n",
    "    b = ed.Normal(loc=6.0, scale=.5, name=\"b\")\n",
    "    mu = a * data + b\n",
    "    return ed.Normal(loc=mu, scale=2.0, name=\"observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_joint = ed.make_log_joint_fn(linear_model)\n",
    "\n",
    "def target_log_prob_fn(a, b):\n",
    "    return log_joint(x, a=a, b=b, observations=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_kernel = tfp.mcmc.MetropolisHastings(\n",
    "    inner_kernel=tfp.mcmc.UncalibratedRandomWalk(target_log_prob_fn=target_log_prob_fn),\n",
    ")\n",
    "states, kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=1000,\n",
    "    current_state=[.0, 2.0],\n",
    "    kernel=mh_kernel,\n",
    "    num_burnin_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    s_, kr_ = sess.run([states, kernel_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(s_[0], bins=10)\n",
    "plt.show();\n",
    "plt.hist(s_[1], bins=10)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, marker=\".\")\n",
    "full_x = np.linspace(-10, 10, 100)\n",
    "for i in range(1000):\n",
    "    pred_y = s_[0][i] * full_x + s_[1][i]\n",
    "    plt.plot(full_x, pred_y, alpha=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus points: have $\\sigma$ as random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
